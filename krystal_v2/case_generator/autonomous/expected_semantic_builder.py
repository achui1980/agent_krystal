"""
Expectedè¯­ä¹‰åº“æ„å»ºå™¨ - è‡ªåŠ¨åˆ†æExpectedå­—æ®µè¯­ä¹‰

è¿™ä¸ªæ¨¡å—ä¼šï¼š
1. è¯»å–expected.txtå¹¶åˆ†ææ‰€æœ‰å­—æ®µ
2. åŸºäºå­—æ®µåå’Œæ ·æœ¬æ•°æ®æ¨æ–­ä¸šåŠ¡å«ä¹‰
3. è¯†åˆ«å¸¸è§æ¨¡å¼å’Œæšä¸¾å€¼
4. æ„å»ºè¯­ä¹‰çŸ¥è¯†åº“ï¼ˆJSONæ ¼å¼ï¼‰
5. ç¼“å­˜ä»¥ä¾¿åç»­ä½¿ç”¨
"""

import os
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional
from crewai import Agent, Task, Crew
from crewai.llm import LLM
from dotenv import load_dotenv


class ExpectedSemanticBuilder:
    """Expectedè¯­ä¹‰åº“æ„å»ºå™¨"""

    def __init__(self, llm: Optional[LLM] = None):
        """
        åˆå§‹åŒ–æ„å»ºå™¨

        Args:
            llm: CrewAI LLMå®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        if llm is None:
            api_key = os.getenv("OPENAI_API_KEY")
            model = os.getenv("OPENAI_MODEL", "gpt-4o")
            llm = LLM(model=model, api_key=api_key) if api_key else None

        self.llm = llm
        self.agent = self._create_semantic_analyst_agent()
        self.cache_dir = Path("./generated_autonomous")
        self.cache_dir.mkdir(exist_ok=True)

    def _create_semantic_analyst_agent(self) -> Agent:
        """åˆ›å»ºè¯­ä¹‰åˆ†æAgent"""
        return Agent(
            role="ETLæ•°æ®è¯­ä¹‰åˆ†æä¸“å®¶",
            goal="åˆ†ææ•°æ®å­—æ®µçš„ä¸šåŠ¡å«ä¹‰ï¼Œæ„å»ºå‡†ç¡®çš„è¯­ä¹‰çŸ¥è¯†åº“",
            backstory="""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•°æ®æ¶æ„å¸ˆå’Œä¸šåŠ¡åˆ†æå¸ˆï¼Œæ‹¥æœ‰15å¹´çš„ETLå’Œæ•°æ®å»ºæ¨¡ç»éªŒã€‚

ä½ çš„æ ¸å¿ƒèƒ½åŠ›ï¼š
1. å­—æ®µå‘½ååˆ†æï¼šé€šè¿‡å­—æ®µåæ¨æ–­ä¸šåŠ¡å«ä¹‰
2. æ•°æ®æ¨¡å¼è¯†åˆ«ï¼šä»æ ·æœ¬æ•°æ®è¯†åˆ«ç±»å‹ã€æ ¼å¼ã€æšä¸¾å€¼
3. ä¸šåŠ¡é€»è¾‘æ¨æ–­ï¼šç†è§£å­—æ®µé—´çš„å…³ç³»å’Œçº¦æŸ
4. é¢†åŸŸçŸ¥è¯†ï¼šç†Ÿæ‚‰åŒ»ç–—ä¿é™©ã€é‡‘èã€ç”µå•†ç­‰å¸¸è§é¢†åŸŸ

åˆ†æåŸåˆ™ï¼š
- åŸºäºè¯æ®ï¼šåªåŸºäºå­—æ®µåå’Œæ ·æœ¬æ•°æ®æ¨æ–­
- ä¿å®ˆæ¨æ–­ï¼šä¸ç¡®å®šæ—¶æ ‡è®°ä¸º"unknown"
- ç»“æ„åŒ–è¾“å‡ºï¼šä½¿ç”¨æ ‡å‡†JSONæ ¼å¼
- è¯¦ç»†æ–‡æ¡£ï¼šä¸ºæ¯ä¸ªå­—æ®µæä¾›æ¸…æ™°çš„æè¿°
            """,
            verbose=True,
            allow_delegation=False,
            llm=self.llm,
        )

    def build_or_load_semantic_map(
        self, expected_path: str, force_rebuild: bool = False
    ) -> Dict[str, Any]:
        """
        æ„å»ºæˆ–åŠ è½½è¯­ä¹‰åº“

        Args:
            expected_path: expected.txtæ–‡ä»¶è·¯å¾„
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰

        Returns:
            è¯­ä¹‰åº“å­—å…¸
        """
        # è®¡ç®—æ–‡ä»¶MD5ä½œä¸ºç¼“å­˜é”®
        file_hash = self._calculate_file_hash(expected_path)
        cache_file = self.cache_dir / f"expected_semantic_map_{file_hash[:8]}.json"

        # æ£€æŸ¥ç¼“å­˜
        if not force_rebuild and cache_file.exists():
            print(f"ğŸ“š åŠ è½½ç¼“å­˜çš„è¯­ä¹‰åº“: {cache_file}")
            with open(cache_file, "r", encoding="utf-8") as f:
                return json.load(f)

        # æ„å»ºæ–°çš„è¯­ä¹‰åº“
        print(f"ğŸ”¨ æ„å»ºExpectedè¯­ä¹‰åº“...")
        semantic_map = self._build_semantic_map(expected_path)

        # ä¿å­˜ç¼“å­˜
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(semantic_map, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ è¯­ä¹‰åº“å·²ç¼“å­˜: {cache_file}")

        return semantic_map

    def _calculate_file_hash(self, filepath: str) -> str:
        """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ"""
        with open(filepath, "rb") as f:
            return hashlib.md5(f.read()).hexdigest()

    def _build_semantic_map(self, expected_path: str) -> Dict[str, Any]:
        """æ„å»ºè¯­ä¹‰åº“"""
        # è¯»å–expected.txt
        expected_data = self._read_expected_file(expected_path)

        # å‡†å¤‡åˆ†æææ–™
        fields = expected_data["headers"]
        sample_rows = expected_data["data"][:5]  # å–å‰5è¡Œæ ·æœ¬

        # Agentåˆ†æ
        task = Task(
            description=f"""
ä½œä¸ºæ•°æ®è¯­ä¹‰åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æExpectedæ•°æ®çš„æ‰€æœ‰å­—æ®µå¹¶æ„å»ºè¯­ä¹‰çŸ¥è¯†åº“ã€‚

**è¾“å…¥æ•°æ®ï¼š**
æ–‡ä»¶: {expected_path}

å­—æ®µåˆ—è¡¨({len(fields)}ä¸ªå­—æ®µ):
{", ".join(fields[:20])}...

æ ·æœ¬æ•°æ®(å‰5è¡Œ):
{json.dumps(sample_rows[:2], ensure_ascii=False, indent=2)}

**åˆ†æä»»åŠ¡ï¼š**

å¯¹æ¯ä¸ªå­—æ®µï¼Œè¯·åˆ†æå¹¶è¾“å‡ºä»¥ä¸‹ä¿¡æ¯ï¼š

1. **business_meaning**: ä¸šåŠ¡å«ä¹‰ï¼ˆåŸºäºå­—æ®µåæ¨æ–­ï¼‰
   - ç¤ºä¾‹: "FIRST_NAME" â†’ "ä¼šå‘˜åå­—"
   - ç¤ºä¾‹: "PRODUCT_LINE" â†’ "äº§å“çº¿ä»£ç "
   - ç¤ºä¾‹: "CARRIER_EFFECTIVE_DATE" â†’ "æ‰¿ä¿ç”Ÿæ•ˆæ—¥æœŸ"

2. **data_type**: æ•°æ®ç±»å‹
   - é€‰é¡¹: string, number, date, boolean, enum
   - å¦‚æœæ˜¯stringï¼Œæ ‡æ³¨é•¿åº¦èŒƒå›´: string(1-50)
   - å¦‚æœæ˜¯dateï¼Œæ ‡æ³¨æ ¼å¼: date(MM/DD/YYYY)

3. **common_patterns**: å¸¸è§æ¨¡å¼/æšä¸¾å€¼ï¼ˆä»æ ·æœ¬æ•°æ®æå–ï¼‰
   - å¦‚æœæ˜¯å·ä»£ç : ["MO", "CA", "NY", "TX", "FL"]
   - å¦‚æœæ˜¯äº§å“ç±»å‹: ["PDP", "LPPO", "HUM", "MD", "MS"]
   - å¦‚æœæ˜¯çŠ¶æ€: ["Active", "Termed"]
   - å¦‚æœæ²¡æœ‰æ˜æ˜¾æ¨¡å¼: []

4. **possible_source_fields**: å¯èƒ½çš„æ¥æºå­—æ®µï¼ˆåŸºäºå‘½åç›¸ä¼¼æ€§ï¼‰
   - ç¤ºä¾‹: "FIRST_NAME" â†’ ["Member", "FIRST_NAME", "Name"]
   - ç¤ºä¾‹: "STATE" â†’ ["State", "STATE"]
   - ä½¿ç”¨å¸¸è§çš„å‘½åå˜ä½“ï¼ˆå¤§å°å†™ã€ä¸‹åˆ’çº¿ã€é©¼å³°ï¼‰

5. **validation_rules**: éªŒè¯è§„åˆ™
   - ç¤ºä¾‹: ["éç©º", "2å­—ç¬¦å·ä»£ç "]
   - ç¤ºä¾‹: ["æ—¥æœŸæ ¼å¼MM/DD/YYYY"]
   - ç¤ºä¾‹: ["å¿…é¡»ä¸CITYå­—æ®µåŒ¹é…"]

6. **derivation_hints**: æ´¾ç”Ÿæç¤ºï¼ˆå¦‚ä½•ä»Sourceè®¡ç®—å¾—åˆ°ï¼‰
   - ç¤ºä¾‹: "FIRST_NAME" â†’ "ä»Memberå­—æ®µæ‹†åˆ†è·å–ï¼ˆæ ¼å¼: LAST,FIRST Mï¼‰"
   - ç¤ºä¾‹: "PRODUCT_LINE" â†’ "æ ¹æ®Productå­—æ®µæ¡ä»¶è½¬æ¢"
   - ç¤ºä¾‹: "IS_ACTIVE" â†’ "æ ¹æ®STATUSå­—æ®µè®¡ç®—ï¼ˆActiveâ†’1, Termedâ†’0ï¼‰"

**è¾“å‡ºæ ¼å¼ï¼š**

è¯·åªè¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š

{{
    "fields": {{
        "CARRIER_STATUS_MAP": {{
            "business_meaning": "æ‰¿ä¿çŠ¶æ€æ˜ å°„",
            "data_type": "string",
            "common_patterns": ["Active", "Termed"],
            "possible_source_fields": ["Policy_Indicator", "STATUS"],
            "validation_rules": ["éç©º", "æšä¸¾å€¼: Active|Termed"],
            "derivation_hints": "ä»Policy_Indicatorå­—æ®µæ˜ å°„"
        }},
        "FIRST_NAME": {{
            "business_meaning": "ä¼šå‘˜åå­—",
            "data_type": "string(1-50)",
            "common_patterns": [],
            "possible_source_fields": ["Member", "FIRST_NAME"],
            "validation_rules": ["å­—æ¯å­—ç¬¦"],
            "derivation_hints": "ä»Memberå­—æ®µæ‹†åˆ†ï¼ˆæ ¼å¼: LAST,FIRST Mï¼‰"
        }},
        ...ï¼ˆå…¶ä½™{len(fields)}ä¸ªå­—æ®µï¼‰
    }},
    "metadata": {{
        "total_fields": {len(fields)},
        "analyzed_samples": {len(sample_rows)},
        "confidence": "high|medium|low"
    }}
}}

**é‡è¦è¦æ±‚ï¼š**
1. å¿…é¡»åˆ†ææ‰€æœ‰{len(fields)}ä¸ªå­—æ®µ
2. å­—æ®µåå¿…é¡»ä¸è¾“å…¥å®Œå…¨ä¸€è‡´
3. ä¸ç¡®å®šæ—¶æ ‡è®°ä¸º"unknown"
4. åªè¾“å‡ºJSONï¼Œä¸è¦markdownä»£ç å—æ ‡è®°
            """,
            expected_output="JSONæ ¼å¼çš„è¯­ä¹‰çŸ¥è¯†åº“",
            agent=self.agent,
        )

        crew = Crew(agents=[self.agent], tasks=[task], verbose=False)
        result = crew.kickoff()

        # è§£æç»“æœ
        try:
            semantic_map = json.loads(result.raw)
            return semantic_map
        except json.JSONDecodeError:
            # å°è¯•æå–JSONï¼ˆå¦‚æœAgentåŒ…å«äº†markdownæ ‡è®°ï¼‰
            raw = result.raw
            if "```json" in raw:
                raw = raw.split("```json")[1].split("```")[0]
            elif "```" in raw:
                raw = raw.split("```")[1].split("```")[0]

            try:
                semantic_map = json.loads(raw.strip())
                return semantic_map
            except:
                print("è­¦å‘Š: æ— æ³•è§£æè¯­ä¹‰åº“JSONï¼Œè¿”å›ç©ºç»“æ„")
                return {"fields": {}, "metadata": {"error": "è§£æå¤±è´¥"}}

    def _read_expected_file(self, expected_path: str) -> Dict[str, Any]:
        """è¯»å–expected.txtæ–‡ä»¶"""
        with open(expected_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        # å‰4è¡Œæ˜¯å…ƒæ•°æ®
        metadata = {}
        for i in range(min(4, len(lines))):
            if ":" in lines[i]:
                key, value = lines[i].strip().split(":", 1)
                metadata[key] = value.strip()

        # ç¬¬5è¡Œæ˜¯è¡¨å¤´
        if len(lines) > 4:
            headers = [h.strip() for h in lines[4].strip().split("|")]
        else:
            headers = []

        # æ•°æ®è¡Œ
        data = []
        for line in lines[5:]:
            if line.strip():
                values = [v.strip() for v in line.strip().split("|")]
                if len(values) == len(headers):
                    row = dict(zip(headers, values))
                    data.append(row)

        return {"metadata": metadata, "headers": headers, "data": data}

    def get_field_semantics(
        self, semantic_map: Dict[str, Any], field_name: str
    ) -> Optional[Dict[str, Any]]:
        """
        è·å–ç‰¹å®šå­—æ®µçš„è¯­ä¹‰ä¿¡æ¯

        Args:
            semantic_map: è¯­ä¹‰åº“
            field_name: å­—æ®µå

        Returns:
            å­—æ®µè¯­ä¹‰ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        return semantic_map.get("fields", {}).get(field_name)

    def search_source_candidates(
        self, semantic_map: Dict[str, Any], source_field: str
    ) -> List[str]:
        """
        æ ¹æ®Sourceå­—æ®µåæœç´¢å¯èƒ½çš„Expectedå­—æ®µ

        Args:
            semantic_map: è¯­ä¹‰åº“
            source_field: Sourceå­—æ®µå

        Returns:
            å¯èƒ½çš„Expectedå­—æ®µåˆ—è¡¨
        """
        candidates = []
        fields = semantic_map.get("fields", {})

        source_field_lower = source_field.lower()

        for expected_field, semantics in fields.items():
            possible_sources = semantics.get("possible_source_fields", [])
            possible_sources_lower = [s.lower() for s in possible_sources]

            if source_field_lower in possible_sources_lower:
                candidates.append(expected_field)

        return candidates


if __name__ == "__main__":
    # æµ‹è¯•è¯­ä¹‰åº“æ„å»ºå™¨
    print("=" * 80)
    print("Expectedè¯­ä¹‰åº“æ„å»ºå™¨æµ‹è¯•")
    print("=" * 80)

    # åŠ è½½ç¯å¢ƒå˜é‡
    env_path = Path(__file__).parent.parent.parent.parent / ".env"
    if env_path.exists():
        load_dotenv(env_path)

    expected_path = "case/expected.txt"

    if not os.path.exists(expected_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {expected_path}")
        print("è¯·ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    else:
        builder = ExpectedSemanticBuilder()

        print(f"\nåˆ†ææ–‡ä»¶: {expected_path}")
        semantic_map = builder.build_or_load_semantic_map(expected_path)

        print(f"\nâœ… è¯­ä¹‰åº“æ„å»ºå®Œæˆ")
        print(f"   æ€»å­—æ®µæ•°: {semantic_map.get('metadata', {}).get('total_fields', 0)}")
        print(f"   å·²åˆ†æå­—æ®µæ•°: {len(semantic_map.get('fields', {}))}")

        # æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹å­—æ®µ
        fields = semantic_map.get("fields", {})
        if fields:
            print(f"\nç¤ºä¾‹å­—æ®µè¯­ä¹‰:")
            for i, (field_name, semantics) in enumerate(list(fields.items())[:3]):
                print(f"\n{i + 1}. {field_name}:")
                print(f"   ä¸šåŠ¡å«ä¹‰: {semantics.get('business_meaning', 'N/A')}")
                print(f"   æ•°æ®ç±»å‹: {semantics.get('data_type', 'N/A')}")
                print(f"   å¸¸è§æ¨¡å¼: {semantics.get('common_patterns', [])}")
