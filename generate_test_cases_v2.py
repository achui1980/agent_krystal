#!/usr/bin/env python3
"""
åŸºäºå®é™…case/ç›®å½•è§„åˆ™çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨
ä½¿ç”¨Source.csvçš„å®é™…å­—æ®µåç”Ÿæˆæ•°æ®
"""

import pandas as pd
import csv
from pathlib import Path
from faker import Faker
import random
from datetime import datetime, timedelta

fake = Faker("en_US")


def generate_test_cases():
    print("=" * 80)
    print("ğŸ“‹ Krystal V2 æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ (åŸºäºå®é™…è§„åˆ™)")
    print("=" * 80)
    print()

    # 1. è¯»å–è§„åˆ™æ–‡ä»¶
    print("ğŸ“ æ­¥éª¤1: è¯»å–è§„åˆ™æ–‡ä»¶...")
    rules_df = pd.read_excel("case/rules.xlsx", sheet_name="Sheet1", header=5)
    print(f"   âœ… è¯»å–äº† {len(rules_df)} æ¡è§„åˆ™")

    # 2. è¯»å–sourceå­—æ®µ
    print("\nğŸ“ æ­¥éª¤2: è¯»å–Sourceå­—æ®µ...")
    with open("case/source.csv", "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        source_headers = next(reader)
    source_headers_clean = [h.strip('"') for h in source_headers]
    print(f"   âœ… Sourceå­—æ®µæ•°: {len(source_headers_clean)}")

    # è·å–æœ‰æ˜ å°„çš„sourceå­—æ®µï¼ˆæ¸…æ´—åçš„ï¼‰
    mapped_sources = set(
        rules_df[rules_df["CARRIER_COLUMN_NAME"].notna()]["CS_COLUMN_NAME"].tolist()
    )

    # åˆ›å»ºå­—æ®µåæ˜ å°„ï¼ˆä»è§„åˆ™å­—æ®µåˆ°Source.csvå­—æ®µçš„è¿‘ä¼¼åŒ¹é…ï¼‰
    field_mapping = {
        "PRODUCT_LINE": "Product",
        "EFFECTIVE_START_DATE": "Eff_Date",
        "CANCELLATION_DATE": "Term_Date",
        "ADDRESS_LINE_1": "Address1",
        "CITY": "City",
        "STATE": "State",
        "ZIP_CODE": "Zip",
        "FIRST_NAME": "Member",
        "LAST_NAME": "Member",
        "BIRTH_DATE": "DOB",
        "MEDICARE_ID": "MEDICARE_ID",
        "CMS_CONTRACT_ID": "Plan_Name",
        "CMS_PLAN_ID": "Plan_Name",
        "AGENCY_NAME": "AOR_Name",
        "AGENCY_ID": "AOR_SAN",
        "AGENT_NAME": "Agent",
        "AGENT_ID": "SAN",
    }

    # æ‰¾å‡ºåœ¨Source.csvä¸­å®é™…å­˜åœ¨çš„ã€æœ‰æ˜ å°„çš„å­—æ®µ
    used_sources = []
    for rule_field, source_field in field_mapping.items():
        if source_field in source_headers_clean:
            used_sources.append(source_field)
    used_sources = list(set(used_sources))  # å»é‡

    print(f"   âœ… æœ‰æ˜ å°„çš„å­—æ®µ: {len(used_sources)} ä¸ª")
    print(f"      {', '.join(used_sources)}")
    print()

    # 3. ç”Ÿæˆæµ‹è¯•æ•°æ®
    print("ğŸ¯ æ­¥éª¤3: ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹...")

    output_dir = Path("generated_autonomous/output")
    output_dir.mkdir(exist_ok=True, parents=True)

    # 3.1 æ­£å¸¸åœºæ™¯ - Sourceæ ¼å¼
    print("   ğŸ“Š ç”Ÿæˆæ­£å¸¸åœºæ™¯Sourceæ•°æ®...")
    source_data = []
    for i in range(10):
        row = generate_source_row(source_headers, source_headers_clean, used_sources)
        source_data.append(row)

    with open(
        output_dir / "test_source_normal.csv", "w", newline="", encoding="utf-8"
    ) as f:
        writer = csv.DictWriter(f, fieldnames=source_headers)
        writer.writeheader()
        writer.writerows(source_data)
    print(f"      âœ… å·²ç”Ÿæˆ test_source_normal.csv ({len(source_data)} æ¡)")

    # 3.2 å¼‚å¸¸åœºæ™¯
    print("   ğŸ“Š ç”Ÿæˆå¼‚å¸¸åœºæ™¯æ•°æ®...")
    abnormal_data = generate_abnormal_cases(
        source_headers, source_headers_clean, used_sources
    )
    with open(
        output_dir / "test_source_abnormal.csv", "w", newline="", encoding="utf-8"
    ) as f:
        writer = csv.DictWriter(f, fieldnames=source_headers)
        writer.writeheader()
        writer.writerows(abnormal_data)
    print(f"      âœ… å·²ç”Ÿæˆ test_source_abnormal.csv ({len(abnormal_data)} æ¡)")

    # 3.3 è¾¹ç•Œåœºæ™¯
    print("   ğŸ“Š ç”Ÿæˆè¾¹ç•Œåœºæ™¯æ•°æ®...")
    boundary_data = generate_boundary_cases(
        source_headers, source_headers_clean, used_sources
    )
    with open(
        output_dir / "test_source_boundary.csv", "w", newline="", encoding="utf-8"
    ) as f:
        writer = csv.DictWriter(f, fieldnames=source_headers)
        writer.writeheader()
        writer.writerows(boundary_data)
    print(f"      âœ… å·²ç”Ÿæˆ test_source_boundary.csv ({len(boundary_data)} æ¡)")

    # 4. ç”ŸæˆExpectedç»“æœ
    print("\nğŸ¯ æ­¥éª¤4: ç”ŸæˆExpectedç»“æœ...")
    expected_normal = [apply_rules(row, rules_df, field_mapping) for row in source_data]
    with open(
        output_dir / "test_expected_normal.csv", "w", newline="", encoding="utf-8"
    ) as f:
        if expected_normal:
            writer = csv.DictWriter(f, fieldnames=list(expected_normal[0].keys()))
            writer.writeheader()
            writer.writerows(expected_normal)
    print(f"      âœ… å·²ç”Ÿæˆ test_expected_normal.csv")

    # 5. è¾“å‡ºæŠ¥å‘Š
    print("\n" + "=" * 80)
    print("âœ… æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå®Œæˆï¼")
    print("=" * 80)
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   ğŸ“„ test_source_normal.csv    - Sourceæ­£å¸¸åœºæ™¯ (10æ¡)")
    print(f"   ğŸ“„ test_source_abnormal.csv   - Sourceå¼‚å¸¸åœºæ™¯ ({len(abnormal_data)}æ¡)")
    print(f"   ğŸ“„ test_source_boundary.csv   - Sourceè¾¹ç•Œåœºæ™¯ ({len(boundary_data)}æ¡)")
    print(f"   ğŸ“„ test_expected_normal.csv   - Expectedæ­£å¸¸åœºæ™¯ (10æ¡)")
    print()

    # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
    print("ğŸ“‹ æ•°æ®æ ·æœ¬:")
    print("-" * 80)
    print("\nSourceæ•°æ® (ç¬¬ä¸€æ¡ï¼Œæœ‰å€¼å­—æ®µ):")
    row = source_data[0]
    for k, v in row.items():
        if v:  # åªæ˜¾ç¤ºæœ‰å€¼çš„å­—æ®µ
            print(f"   {k:25s}: {v}")

    print("\nExpectedæ•°æ® (ç¬¬ä¸€æ¡):")
    for k, v in list(expected_normal[0].items())[:10]:
        print(f"   {k:25s}: {v}")
    if len(expected_normal[0]) > 10:
        print(f"   ... å’Œå¦å¤– {len(expected_normal[0]) - 10} ä¸ªå­—æ®µ")
    print()


def generate_source_row(headers, headers_clean, used_fields):
    """ç”ŸæˆSourceæ ¼å¼çš„ä¸€è¡Œæ•°æ®"""
    row = {}
    state = random.choice(["MO", "CA", "NY", "TX", "FL"])

    for header, header_clean in zip(headers, headers_clean):
        # å¦‚æœå­—æ®µåœ¨è§„åˆ™ä¸­æœ‰æ˜ å°„ï¼Œç”ŸæˆçœŸå®æ•°æ®
        if header_clean in used_fields:
            if "AOR_Name" in header_clean:
                row[header] = fake.company()
            elif "AOR_SAN" in header_clean:
                row[header] = str(random.randint(100000, 999999))
            elif header_clean == "Agent":
                row[header] = fake.name()
            elif header_clean == "SAN":
                row[header] = str(random.randint(100000000, 999999999))
            elif "NPN" in header_clean:
                row[header] = str(random.randint(1000000000, 9999999999))
            elif "Member" in header_clean:
                row[header] = fake.name()
            elif "Address1" in header_clean:
                row[header] = fake.street_address()
            elif "City" in header_clean:
                row[header] = fake.city()
            elif "State" in header_clean:
                row[header] = state
            elif "Zip" in header_clean:
                row[header] = fake.zipcode_in_state(state_abbr=state)
            elif "MEDICARE_ID" in header_clean:
                row[header] = "".join(random.choices("0123456789ABCDEF", k=11))
            elif "DOB" in header_clean:
                row[header] = fake.date_of_birth(
                    minimum_age=18, maximum_age=90
                ).strftime("%Y-%m-%d")
            elif "Product" in header_clean:
                products = ["PDP", "HMO", "PPO", "HUM", "HEZ", "MPZ"]
                row[header] = random.choice(products)
            elif "Plan_Name" in header_clean:
                row[header] = f"Plan {random.randint(100, 999)}"
            elif (
                "Eff_Date" in header_clean
                or "Term_Date" in header_clean
                or "SIGNATURE_DATE" in header_clean
            ):
                row[header] = fake.date_between(
                    start_date="-2y", end_date="+1y"
                ).strftime("%Y-%m-%d")
            elif "UMID" in header_clean:
                row[header] = str(random.randint(10000000, 99999999))
            elif "EndReason" in header_clean:
                reasons = ["Moving", "Dissatisfied", "Deceased", "Other"]
                row[header] = random.choice(reasons)
            elif "PREMIUM" in header_clean:
                row[header] = f"{random.uniform(10, 200):.2f}"
            elif (
                "P2P" in header_clean
                or "LIS" in header_clean
                or "Indicator" in header_clean
            ):
                row[header] = random.choice(["Y", "N"])
            elif (
                "Provider" in header_clean
                or "Published" in header_clean
                or "DOC_ID" in header_clean
            ):
                row[header] = str(random.randint(100000, 999999))
            elif "Solar_Group" in header_clean:
                row[header] = f"GRP{random.randint(100, 999)}"
            else:
                row[header] = fake.word()
        else:
            # å­—æ®µåœ¨è§„åˆ™ä¸­æ²¡æœ‰æ˜ å°„ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
            row[header] = ""

    return row


def generate_abnormal_cases(headers, headers_clean, used_fields):
    """ç”Ÿæˆå¼‚å¸¸åœºæ™¯æ•°æ®"""
    cases = []

    # åœºæ™¯1: ç©ºå§“å
    row = generate_source_row(headers, headers_clean, used_fields)
    for h, hc in zip(headers, headers_clean):
        if "Member" in hc or "Agent" in hc or "AOR_Name" in hc:
            row[h] = ""
    cases.append(row)

    # åœºæ™¯2: æ— æ•ˆæ—¥æœŸ
    row = generate_source_row(headers, headers_clean, used_fields)
    for h, hc in zip(headers, headers_clean):
        if "Date" in hc or "DOB" in hc:
            row[h] = "2025-02-30"
    cases.append(row)

    # åœºæ™¯3: è¶…é•¿åœ°å€
    row = generate_source_row(headers, headers_clean, used_fields)
    for h, hc in zip(headers, headers_clean):
        if "Address" in hc:
            row[h] = "X" * 500
    cases.append(row)

    # åœºæ™¯4: ç‰¹æ®Šå­—ç¬¦
    row = generate_source_row(headers, headers_clean, used_fields)
    for h, hc in zip(headers, headers_clean):
        if "Name" in hc or "Member" in hc:
            row[h] = "Test@#$%^&*()"
    cases.append(row)

    # åœºæ™¯5: ç¼ºå¤±å¿…å¡«å­—æ®µ
    row = generate_source_row(headers, headers_clean, used_fields)
    for h, hc in zip(headers, headers_clean):
        if "MEDICARE_ID" in hc or "Member" in hc:
            row[h] = ""
    cases.append(row)

    return cases


def generate_boundary_cases(headers, headers_clean, used_fields):
    """ç”Ÿæˆè¾¹ç•Œåœºæ™¯æ•°æ®"""
    cases = []

    # åœºæ™¯1: æœ€å°å¹´é¾„
    row = generate_source_row(headers, headers_clean, used_fields)
    for h, hc in zip(headers, headers_clean):
        if "DOB" in hc:
            row[h] = "2007-01-01"
    cases.append(row)

    # åœºæ™¯2: æœ€å¤§å¹´é¾„
    row = generate_source_row(headers, headers_clean, used_fields)
    for h, hc in zip(headers, headers_clean):
        if "DOB" in hc:
            row[h] = "1935-01-01"
    cases.append(row)

    # åœºæ™¯3: é›¶é‡‘é¢
    row = generate_source_row(headers, headers_clean, used_fields)
    for h, hc in zip(headers, headers_clean):
        if "PREMIUM" in hc:
            row[h] = "0.00"
    cases.append(row)

    # åœºæ™¯4: è¶…å¤§é‡‘é¢
    row = generate_source_row(headers, headers_clean, used_fields)
    for h, hc in zip(headers, headers_clean):
        if "PREMIUM" in hc:
            row[h] = "999999.99"
    cases.append(row)

    # åœºæ™¯5: ç©ºå­—ç¬¦ä¸²ï¼ˆæœªä½¿ç”¨å­—æ®µï¼‰
    row = generate_source_row(headers, headers_clean, used_fields)
    for h, hc in zip(headers, headers_clean):
        if hc not in used_fields:
            row[h] = ""
    cases.append(row)

    return cases


def apply_rules(source_row, rules_df, field_mapping):
    """åº”ç”¨è§„åˆ™è½¬æ¢Sourceåˆ°Expected"""
    expected = {}

    # åˆ›å»ºåå‘æ˜ å°„ï¼šSourceå­—æ®µ -> è§„åˆ™å­—æ®µ
    reverse_mapping = {v: k for k, v in field_mapping.items()}

    # éå†Sourceè¡Œä¸­çš„æ¯ä¸ªå­—æ®µ
    for source_header, source_value in source_row.items():
        source_clean = source_header.strip('"')

        # æŸ¥æ‰¾å¯¹åº”çš„è§„åˆ™å­—æ®µ
        if source_clean in reverse_mapping:
            rule_field = reverse_mapping[source_clean]

            # åœ¨è§„åˆ™ä¸­æŸ¥æ‰¾å¯¹åº”çš„Target
            rule_row = rules_df[rules_df["CS_COLUMN_NAME"] == rule_field]
            if not rule_row.empty:
                target_col = rule_row.iloc[0]["CARRIER_COLUMN_NAME"]
                default_val = rule_row.iloc[0]["DEFAULT"]

                if pd.notna(target_col):
                    # å¦‚æœæœ‰ç›®æ ‡å­—æ®µï¼Œè¿›è¡Œæ˜ å°„
                    expected[target_col] = (
                        source_value
                        if source_value
                        else (default_val if pd.notna(default_val) else "")
                    )
                elif pd.notna(default_val):
                    # åªæœ‰é»˜è®¤å€¼
                    expected[rule_field] = default_val

    # æ·»åŠ ä¸€äº›å›ºå®šå­—æ®µ
    expected["ACTION_ID"] = "humana-s10-cs-data-integration"
    expected["SERVICE_MAP_ID"] = "10003358"

    return expected


if __name__ == "__main__":
    generate_test_cases()
